{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos QSAR para protease principal 3C-like protease (M<sup>pro</sup>) de SARS-CoV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Os modelos neste fluxo de trabalho foram gerados usando descritores 2D sirms.py e PaDEL-Descriptor.\n",
    "- Foram coletados 114 pontos de dados para 113 compostos testados em SARS-CoV Mpro (CHEMBL3927). Os dados foram cuidadosamente selecionados seguindo os protocolos desenvolvidos por Fourches et al. Após a curadoria, 91 compostos (27 ativos e 64 inativos) foram mantidos para modelagem.\n",
    "- Devido ao tamanho limitado dos dados, validamos os modelos apenas por meio de validação cruzada externa de 5 vezes e aplicamos um limite de 70% de confiança para avaliar os modelos e selecionar resultados para validação experimental."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importando Módulos e Funções\n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import _pickle as cPickle\n",
    "import gzip\n",
    "from time import time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#BalanceBySim: função de balanceamento de conjunto de dados (Equilibre os dados usando 1/2 similaridade e 1/2 aleatória)\n",
    "from BalanceBySim import *\n",
    "\n",
    "#Stats: Calculos estatisticos referente ao modelo Kappa\tCCR\tSensitivity\tPPV\tSpecificity\tNPV\tCoverage\n",
    "from stats import *\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "#Rdkit: coleção de quiminformática e software de aprendizado de máquina escrito em C++ e Python de Código Aberto.\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import PandasTools\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "\n",
    "#Sklearn: Bibliotecas p/ Machine learning de Código Aberto\n",
    "from sklearn import metrics\n",
    "from sklearn.svm import SVC # biblioteca SVM para Classificação\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV, RandomizedSearchCV, cross_val_score # biblioteca GridSearch e cross_validate\n",
    "from sklearn.model_selection import permutation_test_score, StratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn = warn\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "Draw.DrawingOptions.atomLabelFontFace = \"DejaVu Sans\"\n",
    "Draw.DrawingOptions.atomLabelFontSize = 18\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "hiper_parametros = 'grid_search' #grid_search, random_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir caminho do arquivo\n",
    "file = 'datasets/curated_data/chembl-pdb-sars-cov-3C-like-proteinase.sdf.gz'\n",
    "\n",
    "# Novo dicionário inicializado a partir de um objeto de mapeamento\n",
    "sdfInfo = dict(smilesName='SMILES', molColName='ROMol')\n",
    "\n",
    "# Carregando o arquivo SDF com os dicionarios mapeados\n",
    "moldf = PandasTools.LoadSDF(file, **sdfInfo)\n",
    "print('Dados originais: ', moldf.shape)\n",
    "\n",
    "# Renomear ROMol\n",
    "moldf = moldf.rename(columns={'ROMol': 'Mol'})\n",
    "\n",
    "# Remover moléculas RDKit ausentes\n",
    "moldf = moldf[pd.notnull(moldf['Mol'])]\n",
    "if 'StandardizerResult' in moldf.columns:\n",
    "    moldf = moldf.drop(columns='StandardizerResult')\n",
    "    \n",
    "# Colunas\n",
    "print('Dados mantidos: ', moldf.shape)\n",
    "moldf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forma dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# (27 ativos e 64 inativos) 91 compostos utilizando o software ChemAxon Standardizer \n",
    "# (13 ativos e 09 inativos) 22 compostos obtidos de empresas encontradas do PDB\n",
    "moldf['Outcome'] = moldf['Outcome'].replace('Active', 1)\n",
    "moldf['Outcome'] = moldf['Outcome'].replace('Inactive', 0)\n",
    "\n",
    "classes = Counter(moldf['Outcome'])\n",
    "print('\\033[1m' + 'Forma do conjunto de treinamento:' + '\\n' + '\\033[0m')\n",
    "for key, value in classes.items():\n",
    "    print('\\t\\t Classe %d: %d' % (key, value))\n",
    "print('\\t\\t Número total de compostos: %d' % (len(moldf['Outcome'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SiRMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descritores de importação\n",
    "\n",
    "Os descritores foram calculados externamente usando o SiRMS.py. Descritores com baixa variância e correlacionados foram removidos do http://www.qsar4u.com/pages/sirms.php usando o módulo Métodos de Análise de Dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "desc = pd.read_csv('descriptors/sirms_descritores.csv', sep='\\t')\n",
    "desc.drop(desc.columns[0:2], axis=1,inplace=True)\n",
    "\n",
    "#Retorne um novo índice com elementos do índice que não estão no \"outro\".\n",
    "descriptors = desc.columns.difference(moldf.columns).tolist()\n",
    "desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moldf_desc = pd.concat([moldf,desc], axis=1)\n",
    "balance_data = 'no'\n",
    "\n",
    "if balance_data == 'yes':\n",
    "    # Equilibre os dados usando 1/2 similaridade e 1/2 aleatória\n",
    "    moldf_desc = BalanceBySim(moldf_desc, 'Outcome', 2)\n",
    "    # Formas dos conjuntos\n",
    "    print('Forma do conjunto de treinamento: %s' % Counter(moldf_desc['Outcome'].loc[moldf_desc['Set'] == 'train']))\n",
    "    print('Forma externa definida: %s' % Counter(moldf_desc['Outcome'].loc[moldf_desc['Set'] == 'ext']))\n",
    "      \n",
    "else:\n",
    "    moldf_desc['Set'] = 'train'\n",
    "    # Formas dos conjuntos\n",
    "    print('Forma do conjunto de treinamento: %s' % Counter(moldf_desc['Outcome']))\n",
    "    print('Forma externa definida: %s' % Counter(moldf_desc['Outcome'].loc[moldf_desc['Set'] == 'ext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#conjunto de treinamento\n",
    "moldf_train = moldf_desc[(moldf_desc['Set'] == 'train')]\n",
    "\n",
    "# variáveis dependentes ()\n",
    "y_train = moldf_train['Outcome'].to_numpy()\n",
    "\n",
    "# variáceis independentes - propriedades estruturais (descritores) calculados\n",
    "X_train = moldf_train[descriptors]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remova variáveis constantes e quase constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.select_dtypes(exclude=['object'])\n",
    "X_train = X_train.dropna(axis=1, how='any')\n",
    "X_train = X_train.fillna(0)\n",
    "\n",
    "# Definir filtro de baixa variação (limite de 10%)\n",
    "def variance_filter(data, threshold=0.1):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Aplicar filtro\n",
    "X_train = variance_filter(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover variáveis correlacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "correlated_features = set()  \n",
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "X_train.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('descriptors/sirms-chembl-sars-cov-3C-like-proteinase-processed.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Modelo com classificação: {0}\".format(i))\n",
    "            print(\"Escore médio de validação: {0:.3f} (std: {1:.3f})\"\n",
    "                  .format(results['mean_test_score'][candidate],\n",
    "                          results['std_test_score'][candidate]))\n",
    "            print(\"Parametros: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encontre os melhores parâmetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesquisa em grade (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(clf_rf, rf_params, X_train, y_train):\n",
    "    n_iter_search = 20\n",
    "    # configuração detalhada = 10 imprimirá o progresso para cada 10 tarefas concluídas\n",
    "    grid = GridSearchCV(clf_rf, rf_params, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
    "    \n",
    "    start = time()\n",
    "    grid.fit(X_train, y_train)\n",
    "    print(\"GridSearchCV levou %.2f segundos para %d candidatos\"\n",
    "      \" configurações de parâmetros.\" % ((time() - start), n_iter_search))\n",
    "    report(grid.cv_results_)\n",
    "\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pesquisa aleatória (Random Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(clf_rf, param_dist, X_train, y_train):\n",
    "    n_iter_search = 80\n",
    "    random_search = RandomizedSearchCV(clf_rf, param_distributions=param_dist, \n",
    "                                       n_iter=n_iter_search, verbose=1, n_jobs=-1)\n",
    "    \n",
    "    start = time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    print(\"RandomizedSearchCV levou %.2f segundos para %d candidatos\"\n",
    "      \" configurações de parâmetros.\" % ((time() - start), n_iter_search))\n",
    "    report(random_search.cv_results_)\n",
    "\n",
    "    return random_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['rbf', 'linear', 'sigmoid', 'poly']\n",
    "C = [1, 1000]\n",
    "gamma = [1, 0.00001]\n",
    "random_state = [0, 24]\n",
    "\n",
    "# Crie a grade aleatória\n",
    "random_param = {'kernel': kernel,\n",
    "              'C': C,\n",
    "              'gamma': gamma,\n",
    "              'random_state': random_state}\n",
    "\n",
    "#Normalize\n",
    "#X = StandardScaler().fit_transform(X)\n",
    "clf_svc_sirms = SVC(random_state=24, probability=True).fit(X_train, y_train)\n",
    "\n",
    "svc_random_sirms = random_search(clf_svc_sirms, random_param, X_train, y_train);\n",
    "\n",
    "grid_params_sirms = {\n",
    "    'kernel': [svc_random_sirms.best_params_['kernel']],\n",
    "    'random_state': [svc_random_sirms.best_params_['random_state']], \n",
    "    'gamma': [svc_random_sirms.best_params_['gamma'] - 0.1, \n",
    "         svc_random_sirms.best_params_['gamma'] - 0.01, \n",
    "         svc_random_sirms.best_params_['gamma'], \n",
    "         svc_random_sirms.best_params_['gamma'] + 0.1, \n",
    "         svc_random_sirms.best_params_['gamma'] + 0.01,\n",
    "             'auto'],\n",
    "    'C': [svc_random_sirms.best_params_['C'] - 150, \n",
    "         svc_random_sirms.best_params_['C'] - 100, \n",
    "         svc_random_sirms.best_params_['C'], \n",
    "         svc_random_sirms.best_params_['C'] + 100, \n",
    "         svc_random_sirms.best_params_['C'] + 150]\n",
    "}\n",
    "\n",
    "svc_grid_sirms = grid_search(clf_svc_sirms, grid_params_sirms, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cross_val_score(clf_svc_sirms, X_train, y_train).mean()\n",
    "y_gs = np.maximum.accumulate(svc_grid_sirms.cv_results_['mean_test_score'])\n",
    "y_rs = np.maximum.accumulate(svc_random_sirms.cv_results_['mean_test_score'])\n",
    "\n",
    "print(f'Baseline = {baseline:.2f}')\n",
    "\n",
    "print()\n",
    "print('Best param - Random search: %s' % svc_random_sirms.best_params_)\n",
    "print(f'Random search = %.2f' % svc_random_sirms.best_score_)\n",
    "\n",
    "print()\n",
    "print('Best param - Grid search: %s' % svc_grid_sirms.best_params_)\n",
    "print(f'Grid search = %.2f' % svc_grid_sirms.best_score_)\n",
    "\n",
    "plt.plot(y_gs, 'gs-', label='Grid search')\n",
    "plt.plot(y_rs, 'rs-', label='Random search')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('score')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Valor da melhor pontuação CV amostrada');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modelo com w/ melhores parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best = SVC(C=101, gamma='auto', random_state= 0, kernel='poly', probability=True)\n",
    "svc_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "pred = []\n",
    "ad = []\n",
    "index = []\n",
    "cross_val = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Faça um loop de 5 vezes\n",
    "for train_index, test_index in cross_val.split(X_train, y_train):    \n",
    "    fold_model = svc_best.fit(X_train.iloc[train_index], y_train[train_index])\n",
    "    fold_pred = svc_best.predict(X_train.iloc[test_index])\n",
    "    fold_ad = svc_best.predict_proba(X_train.iloc[test_index])\n",
    "    pred.append(fold_pred)\n",
    "    ad.append(fold_ad)\n",
    "    index.append(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_ad = 0.70\n",
    "\n",
    "# Preparar resultados para exportar   \n",
    "fold_index = np.concatenate(index)    \n",
    "fold_pred = np.concatenate(pred)\n",
    "fold_ad = np.concatenate(ad)\n",
    "fold_ad = (np.amax(fold_ad, axis=1) >= threshold_ad).astype(str)\n",
    "five_fold_sirms = pd.DataFrame({'Prediction': fold_pred,'AD': fold_ad}, index=list(fold_index))\n",
    "five_fold_sirms.AD[five_fold_sirms.AD == 'False'] = np.nan\n",
    "five_fold_sirms.AD[five_fold_sirms.AD == 'True'] = five_fold_sirms.Prediction\n",
    "five_fold_sirms.sort_index(inplace=True)\n",
    "five_fold_sirms['y_train'] = pd.DataFrame(y_train)\n",
    "five_fold_sirms_ad = five_fold_sirms.dropna().astype(int)\n",
    "cobertura_5f = len(five_fold_sirms_ad) / len(five_fold_sirms)\n",
    "\n",
    "# estatísticas de sirms\n",
    "sirms = pd.DataFrame(stats(five_fold_sirms['y_train'], five_fold_sirms['Prediction']))\n",
    "sirms['Cobertura'] = 1.0\n",
    "\n",
    "# estatísticas de sirms AD\n",
    "sirms_ad = five_fold_sirms.dropna(subset=['AD']).astype(int)\n",
    "cobertura_sirms_ad = len(sirms_ad['AD']) / len(five_fold_sirms['y_train'])\n",
    "sirms_ad = pd.DataFrame(stats(sirms_ad['y_train'], sirms_ad['AD']))\n",
    "sirms_ad['Cobertura'] = round(cobertura_sirms_ad, 2)\n",
    "\n",
    "# imprimir estatísticas\n",
    "print('\\033[1m' + 'Características estatísticas de validação cruzada externa de 5 vezes dos modelos QSAR desenvolvidos SiRMS' + '\\n' + '\\033[0m')\n",
    "sirms_5f_stats = sirms.append(sirms_ad)\n",
    "sirms_5f_stats.set_index([['SiRMS', 'SiRMS AD']], drop=True, inplace=True)\n",
    "sirms_5f_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prever conjunto retido externo após o balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moldf_ext = moldf_desc[(moldf_desc['Set'] == 'ext')]\n",
    "descriptor_list = list(X_train.columns.values)\n",
    "\n",
    "if len(moldf_ext) > 0:\n",
    "    y_ext = moldf_ext['Outcome'].to_numpy()\n",
    "    X_ext = moldf_ext[descriptors]\n",
    "    \n",
    "    # Filtrar descritores não presentes no modelo\n",
    "    X_ext = X_ext[descriptor_list]\n",
    "    \n",
    "    # Fazer previsões\n",
    "    ext_set_sirms = svc_best.predict(X_ext)\n",
    "    ext_set_sirms_ad = svc_best.predict_proba(X_ext)\n",
    "    ext_set_sirms_ad = (np.amax(ext_set_sirms_ad, axis=1) >= threshold_ad).astype(str)\n",
    "    \n",
    "    # Preparar dados\n",
    "    ext_set_sirms = pd.DataFrame({'Prediction': ext_set_sirms,'AD': ext_set_sirms_ad})\n",
    "    ext_set_sirms.AD[ext_set_sirms.AD == 'False'] = np.nan\n",
    "    ext_set_sirms.AD[ext_set_sirms.AD == 'True'] = ext_set_sirms.Prediction\n",
    "    ext_set_sirms.sort_index(inplace=True)\n",
    "    ext_set_sirms['y_ext'] = pd.DataFrame(y_ext)\n",
    "    ext_set_sirms_ad = ext_set_sirms.dropna().astype(int)\n",
    "    cobertura_ext = len(ext_set_sirms_ad) / len(ext_set_sirms)\n",
    "    \n",
    "    # ext_set_sirms estatísticas\n",
    "    sirms_ext = pd.DataFrame(stats(ext_set_sirms['y_ext'], ext_set_sirms['Prediction']))\n",
    "    sirms_ext['Cobertura'] = 1.0\n",
    "    \n",
    "    # ext_set_sirms AD estatísticas\n",
    "    sirms_ext_ad = ext_set_sirms.dropna(subset=['AD']).astype(int)\n",
    "    cobertura_sirms_ext_ad = len(sirms_ext_ad['AD']) / len(ext_set_sirms['y_ext'])\n",
    "    sirms_ext_ad = pd.DataFrame(stats(sirms_ext_ad['y_ext'], sirms_ext_ad['AD']))\n",
    "    sirms_ext_ad['Cobertura'] = round(cobertura_sirms_ext_ad, 2)\n",
    "    \n",
    "    # imprimir estatísticas\n",
    "    print('\\033[1m' + 'Características estatísticas da previsão do conjunto retido na fonte por modelos SiRMS' + '\\n' + '\\033[0m')\n",
    "    ext_set_sirms_stats = sirms_ext.append(sirms_ext_ad)\n",
    "    ext_set_sirms_stats.set_index([['SiRMS Ext.', 'SiRMS Ext. AD']], drop=True, inplace=True)\n",
    "    ext_set_sirms_stats\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Y-randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = 20\n",
    "score, permutation_scores, pvalue = permutation_test_score(svc_best, X_train, y_train,\n",
    "                                                           cv=5, scoring='balanced_accuracy',\n",
    "                                                           n_permutations=permutations,\n",
    "                                                           n_jobs=-1,\n",
    "                                                           verbose=1,\n",
    "                                                           random_state=24)\n",
    "print('True score = ', score.round(2),\n",
    "      '\\nY-randomization = ', np.mean(permutation_scores).round(2),\n",
    "      '\\np-value = ', pvalue.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.GzipFile('model/sars-cov-3clpro-sirms_SVM_ad_balanced.pgz', 'w') as f:\n",
    "    cPickle.dump(svc_best, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estatísticas de plotagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar estatísticas\n",
    "if len(moldf_ext) > 0:\n",
    "    sirms_stats = pd.concat([sirms_5f_stats, ext_set_sirms_stats], axis=0)\n",
    "    sirms_stats\n",
    "else:\n",
    "    sirms_stats = sirms_5f_stats.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas de transposição\n",
    "sirms_stats_t = sirms_stats.T\n",
    "sirms_stats_t = sirms_stats_t.reset_index()\n",
    "sirms_stats_t = sirms_stats_t.rename(columns={'index': 'Stats'})\n",
    "\n",
    "# Fazer enredo\n",
    "plt.style.use('seaborn-colorblind')\n",
    "fig, ax1 = plt.subplots(figsize=(8,5), dpi=130)\n",
    "\n",
    "sirms_stats_t.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_xticklabels(labels=sirms_stats_t['Stats'].tolist(), fontsize=14, rotation=0)\n",
    "ax1.axhline(y=.6, color='indianred', ls='dashed')\n",
    "ax1.legend_.remove()\n",
    "plt.title('Características estatísticas', fontsize=16)\n",
    "ax1.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax1.tick_params(labelsize=9)\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles, labels, fontsize=12,\n",
    "            loc='upper center', bbox_to_anchor=(0.5, -0.09), ncol=4)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('statistics-sirms.png', bbox_inches='tight', transparent=False, format='png', dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PaDEL-Descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descritores de importação\n",
    "\n",
    "Os descritores foram calculados externamente usando o PaDEL-Descriptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc = pd.read_csv('descriptors/descritores_padel.csv', sep=',')\n",
    "desc.drop(desc.columns[0:1], axis=1,inplace=True)\n",
    "descriptors = desc.columns.difference(moldf.columns).tolist()\n",
    "desc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moldf_desc = pd.concat([moldf,desc], axis=1)\n",
    "balance_data = 'no'\n",
    "\n",
    "if balance_data == 'yes':\n",
    "    # Equilibre os dados usando 1/2 similaridade e 1/2 aleatória\n",
    "    moldf_desc = BalanceBySim(moldf_desc, 'Outcome', 2)\n",
    "    # Forma de impressão\n",
    "    print('Forma do conjunto de treinamento: %s' % Counter(moldf_desc['Outcome'].loc[moldf_desc['Set'] == 'train']))\n",
    "    print('Forma externa definida: %s' % Counter(moldf_desc['Outcome'].loc[moldf_desc['Set'] == 'ext']))\n",
    "      \n",
    "else:\n",
    "    moldf_desc['Set'] = 'train'\n",
    "    # Forma de impressão\n",
    "    print('Forma do conjunto de treinamento: %s' % Counter(moldf_desc['Outcome']))\n",
    "    print('Forma externa definida: %s' % Counter(moldf_desc['Outcome'].loc[moldf_desc['Set'] == 'ext']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moldf_train = moldf_desc[(moldf_desc['Set'] == 'train')]\n",
    "\n",
    "y_train = moldf_train['Outcome'].to_numpy()\n",
    "X_train = moldf_train[descriptors]\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover variáveis constantes e quase constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.select_dtypes(exclude=['object'])\n",
    "X_train = X_train.dropna(axis=1, how='any')\n",
    "X_train = X_train.fillna(0)\n",
    "\n",
    "# Definir filtro de baixa variação (limite de 10%)\n",
    "def variance_filter(data, threshold=0.1):\n",
    "    selector = VarianceThreshold(threshold)\n",
    "    selector.fit(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "# Aplicar filtro\n",
    "X_train = variance_filter(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remover variáveis correlacionadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = set()  \n",
    "correlation_matrix = X_train.corr()\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):  \n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.9:\n",
    "            colname = correlation_matrix.columns[i]\n",
    "            correlated_features.add(colname)\n",
    "\n",
    "X_train.drop(labels=correlated_features, axis=1, inplace=True)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('descriptors/padel-chembl-sars-cov-3C-like-proteinase-processed.txt', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construção de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dados de modelagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Encontre os melhores parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf_svc_padel = SVC(random_state=24, probability = True).fit(X_train, y_train)\n",
    "\n",
    "svc_random_padel = random_search(clf_svc_padel, random_param, X_train, y_train);\n",
    "\n",
    "grid_params_padel = {\n",
    "    'kernel': [svc_random_padel.best_params_['kernel']],\n",
    "    'random_state': [svc_random_padel.best_params_['random_state']], \n",
    "    'gamma': [svc_random_padel.best_params_['gamma'] - 0.1, \n",
    "         svc_random_padel.best_params_['gamma'] - 0.01, \n",
    "         svc_random_padel.best_params_['gamma'], \n",
    "         svc_random_padel.best_params_['gamma'] + 0.1, \n",
    "         svc_random_padel.best_params_['gamma'] + 0.01,\n",
    "             'auto'],\n",
    "    'C': [svc_random_padel.best_params_['C'] - 150, \n",
    "         svc_random_padel.best_params_['C'] - 100, \n",
    "         svc_random_padel.best_params_['C'], \n",
    "         svc_random_padel.best_params_['C'] + 100, \n",
    "         svc_random_padel.best_params_['C'] + 150]\n",
    "}\n",
    "\n",
    "svc_grid_padel = grid_search(clf_svc_padel, grid_params_padel, X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = cross_val_score(clf_svc_padel, X_train, y_train).mean()\n",
    "y_gs = np.maximum.accumulate(svc_grid_padel.cv_results_['mean_test_score'])\n",
    "y_rs = np.maximum.accumulate(svc_random_padel.cv_results_['mean_test_score'])\n",
    "\n",
    "print(f'Baseline = {baseline:.2f}')\n",
    "\n",
    "print()\n",
    "print('Best param - Random search: %s' % svc_random_padel.best_params_)\n",
    "print(f'Random search = %.2f' % svc_random_padel.best_score_)\n",
    "\n",
    "print()\n",
    "print('Best param - Grid search: %s' % svc_grid_padel.best_params_)\n",
    "print(f'Grid search = %.2f' % svc_grid_padel.best_score_)\n",
    "\n",
    "plt.plot(y_gs, 'gs-', label='Grid search')\n",
    "plt.plot(y_rs, 'rs-', label='Random search')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('score')\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Valor da melhor pontuação CV amostrada');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_best = SVC(C=1, gamma=0.9, random_state= 0, kernel='linear', probability=True)\n",
    "svc_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validação cruzada 5 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametros\n",
    "pred = []\n",
    "ad = []\n",
    "index = []\n",
    "cross_val = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# Faça um loop de 5 vezes\n",
    "for train_index, test_index in cross_val.split(X_train, y_train):\n",
    "    \n",
    "    fold_model = svc_best.fit(X_train.iloc[train_index], y_train[train_index])\n",
    "    fold_pred = svc_best.predict(X_train.iloc[test_index])\n",
    "    fold_ad = svc_best.predict_proba(X_train.iloc[test_index])\n",
    "    pred.append(fold_pred)\n",
    "    ad.append(fold_ad)\n",
    "    index.append(test_index)\n",
    "\n",
    "threshold_ad = 0.70\n",
    "\n",
    "# Preparar resultados para exportar    \n",
    "fold_index = np.concatenate(index)    \n",
    "fold_pred = np.concatenate(pred)\n",
    "fold_ad = np.concatenate(ad)\n",
    "fold_ad = (np.amax(fold_ad, axis=1) >= threshold_ad).astype(str)\n",
    "five_fold_padel = pd.DataFrame({'Prediction': fold_pred,'AD': fold_ad}, index=list(fold_index))\n",
    "five_fold_padel.AD[five_fold_padel.AD == 'False'] = np.nan\n",
    "five_fold_padel.AD[five_fold_padel.AD == 'True'] = five_fold_padel.Prediction\n",
    "five_fold_padel.sort_index(inplace=True)\n",
    "five_fold_padel['y_train'] = pd.DataFrame(y_train)\n",
    "five_fold_padel_ad = five_fold_padel.dropna().astype(int)\n",
    "cobertura_5f = len(five_fold_padel_ad) / len(five_fold_padel)\n",
    "\n",
    "# estatísticas padel\n",
    "padel = pd.DataFrame(stats(five_fold_padel['y_train'], five_fold_padel['Prediction']))\n",
    "padel['Cobertura'] = 1.0\n",
    "\n",
    "# estatísticas padel AD\n",
    "padel_ad = five_fold_padel.dropna(subset=['AD']).astype(int)\n",
    "cobertura_padel_ad = len(padel_ad['AD']) / len(five_fold_padel['y_train'])\n",
    "padel_ad = pd.DataFrame(stats(padel_ad['y_train'], padel_ad['AD']))\n",
    "padel_ad['Cobertura'] = round(cobertura_padel_ad, 2)\n",
    "\n",
    "# imprimir estatísticas\n",
    "print('\\033[1m' + 'Características estatísticas de validação cruzada externa de 5 vezes dos modelos QSAR desenvolvidos PaDEL' + '\\n' + '\\033[0m')\n",
    "padel_5f_stats = padel.append(padel_ad)\n",
    "padel_5f_stats.set_index([['PaDEL', 'PaDEL AD']], drop=True, inplace=True)\n",
    "padel_5f_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prever conjunto retido externo após o balanceamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moldf_ext = moldf_desc[(moldf_desc['Set'] == 'ext')]\n",
    "descriptor_list = list(X_train.columns.values)\n",
    "\n",
    "if len(moldf_ext) > 0:\n",
    "    y_ext = moldf_ext['Outcome'].to_numpy()\n",
    "    X_ext = moldf_ext[descriptors]\n",
    "    \n",
    "    # Filtrar descritores não presentes no modelo\n",
    "    X_ext = X_ext[descriptor_list]\n",
    "    \n",
    "    # Fazer previsões\n",
    "    ext_set_padel = svc_best.predict(X_ext)\n",
    "    ext_set_padel_ad = svc_best.predict_proba(X_ext)\n",
    "    ext_set_padel_ad = (np.amax(ext_set_padel_ad, axis=1) >= threshold_ad).astype(str)\n",
    "    \n",
    "    # Preparar dados\n",
    "    ext_set_padel = pd.DataFrame({'Prediction': ext_set_padel,'AD': ext_set_padel_ad})\n",
    "    ext_set_padel.AD[ext_set_padel.AD == 'False'] = np.nan\n",
    "    ext_set_padel.AD[ext_set_padel.AD == 'True'] = ext_set_padel.Prediction\n",
    "    ext_set_padel.sort_index(inplace=True)\n",
    "    ext_set_padel['y_ext'] = pd.DataFrame(y_ext)\n",
    "    ext_set_padel_ad = ext_set_padel.dropna().astype(int)\n",
    "    cobertura_ext = len(ext_set_padel_ad) / len(ext_set_padel)\n",
    "    \n",
    "    # ext_set_padel estatísticas\n",
    "    padel_ext = pd.DataFrame(stats(ext_set_padel['y_ext'], ext_set_padel['Prediction']))\n",
    "    padel_ext['Cobertura'] = 1.0\n",
    "    \n",
    "    # ext_set_padel AD stats\n",
    "    padel_ext_ad = ext_set_padel.dropna(subset=['AD']).astype(int)\n",
    "    cobertura_padel_ext_ad = len(padel_ext_ad['AD']) / len(ext_set_padel['y_ext'])\n",
    "    padel_ext_ad = pd.DataFrame(stats(padel_ext_ad['y_ext'], padel_ext_ad['AD']))\n",
    "    padel_ext_ad['Cobertura'] = round(cobertura_padel_ext_ad, 2)\n",
    "    \n",
    "    # imprimir estatísticas\n",
    "    print('\\033[1m' + 'Características estatísticas da previsão do conjunto retido na fonte por modelos PaDEL' + '\\n' + '\\033[0m')\n",
    "    ext_set_padel_stats = padel_ext.append(padel_ext_ad)\n",
    "    ext_set_padel_stats.set_index([['PaDEL Ext.', 'PaDEL Ext. AD']], drop=True, inplace=True)\n",
    "    ext_set_padel_stats\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Y-randomization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permutations = 20\n",
    "score, permutation_scores, pvalue = permutation_test_score(svc_best, X_train, y_train,\n",
    "                                                           cv=5, scoring='balanced_accuracy',\n",
    "                                                           n_permutations=permutations,\n",
    "                                                           n_jobs=-1,\n",
    "                                                           verbose=1,\n",
    "                                                           random_state=24)\n",
    "print('True score = ', score.round(2),\n",
    "      '\\nY-randomization = ', np.mean(permutation_scores).round(2),\n",
    "      '\\np-value = ', pvalue.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Salvar modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.GzipFile('model/sars-cov-3clpro-padel_SVM_ad_balanced.pgz', 'w') as f:\n",
    "    cPickle.dump(svc_best, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotar estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportar estatísticas\n",
    "if len(moldf_ext) > 0:\n",
    "    padel_stats = pd.concat([padel_5f_stats, ext_set_padel_stats], axis=0)\n",
    "    padel_stats\n",
    "else:\n",
    "    padel_stats = padel_5f_stats.copy()\n",
    "    padel_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas de transposição\n",
    "padel_stats_t = padel_stats.T\n",
    "padel_stats_t = padel_stats_t.reset_index()\n",
    "padel_stats_t = padel_stats_t.rename(columns={'index': 'Stats'})\n",
    "\n",
    "# Fazer enredo\n",
    "plt.style.use('seaborn-colorblind')\n",
    "fig, ax1 = plt.subplots(figsize=(8,5), dpi=130)\n",
    "\n",
    "padel_stats_t.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_xticklabels(labels=padel_stats_t['Stats'].tolist(), fontsize=14, rotation=0)\n",
    "ax1.axhline(y=.6, color='indianred', ls='dashed')\n",
    "ax1.legend_.remove()\n",
    "plt.title('Características estatísticas', fontsize=16)\n",
    "ax1.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax1.tick_params(labelsize=12)\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles, labels, fontsize=12,\n",
    "            loc='upper center', bbox_to_anchor=(0.5, -0.09), ncol=4)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('statistics-padel.png', bbox_inches='tight', transparent=False, format='png', dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sirms = five_fold_sirms.drop(columns='y_train')\n",
    "results_sirms = five_fold_sirms.rename(columns={'Prediction':'sirms', 'AD':'sirms_ad'})\n",
    "results_padel = five_fold_padel.drop(columns='y_train')\n",
    "results_padel = five_fold_padel.rename(columns={'Prediction':'padel', 'AD':'padel_ad'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var = list(moldf.columns.values)\n",
    "moldf_train = moldf_train[var]\n",
    "results_sirms.reset_index(drop=True, inplace=True)\n",
    "results_padel.reset_index(drop=True, inplace=True)\n",
    "predictions = pd.concat([moldf_train.reset_index(drop=True), results_sirms, results_padel], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Previsões de consenso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consenso\n",
    "predictions['consensus'] = (predictions.sirms + predictions.padel)/2\n",
    "predictions['consensus'] = np.where(predictions['consensus'] > 0.5, 1, 0)\n",
    "\n",
    "# Consenso AD\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    if all([np.isnan(predictions.sirms_ad[i]) == False, np.isnan(predictions.padel_ad[i]) == False]):\n",
    "        predictions.loc[i,'consensus_ad'] = (predictions.sirms_ad[i] + predictions.padel_ad[i])/2\n",
    "        predictions.loc[i,'consensus_ad'] = np.where(predictions.loc[i,'consensus_ad'] > 0.5, 1, 0)\n",
    "    elif all([np.isnan(predictions.sirms_ad[i]) == True, np.isnan(predictions.padel_ad[i]) == False]):\n",
    "        predictions.loc[i,'consensus_ad'] = predictions.padel_ad[i]\n",
    "    elif all([np.isnan(predictions.sirms_ad[i]) == False, np.isnan(predictions.padel_ad[i]) == True]):\n",
    "        predictions.loc[i,'consensus_ad'] = predictions.sirms_ad[i]\n",
    "    else:\n",
    "        predictions.loc[i,'consensus_ad']  = np.nan\n",
    "\n",
    "# Rigor de consenso\n",
    "for i in range(0, predictions.shape[0]):\n",
    "    if all([np.isnan(predictions.sirms_ad[i]) == False, np.isnan(predictions.padel_ad[i]) == False]):\n",
    "        predictions.loc[i,'consensus_rigor'] = (predictions.sirms_ad[i] + predictions.padel_ad[i])/2\n",
    "        predictions.loc[i,'consensus_rigor'] = np.where(predictions.loc[i,'consensus_rigor'] > 0.5, 1, 0)\n",
    "    else:\n",
    "        predictions.loc[i,'consensus_rigor']  = np.nan\n",
    "        \n",
    "predictions.drop(columns=['y_train', 'ID'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### SiRMS\n",
    "predictions.sirms = predictions.sirms.fillna(predictions.sirms.median())\n",
    "# Estatísticas SiRMS\n",
    "sirms = pd.DataFrame(stats(predictions.Outcome, predictions.sirms))\n",
    "sirms['Cobertura'] = 1.0\n",
    "\n",
    "# Estatísticas SiRMS AD\n",
    "sirms_ad = predictions.dropna(subset=['sirms_ad'])\n",
    "cobertura_sirms_ad = len(sirms_ad.sirms_ad) / len(predictions.Outcome)\n",
    "sirms_ad = pd.DataFrame(stats(sirms_ad.Outcome, sirms_ad.sirms_ad.astype(int)))\n",
    "sirms_ad['Cobertura'] = round(cobertura_sirms_ad, 2)\n",
    "\n",
    "##### PaDEL\n",
    "predictions.padel = predictions.padel.fillna(predictions.padel.median())\n",
    "# estatísticas de padel\n",
    "padel = pd.DataFrame(stats(predictions.Outcome, predictions.padel))\n",
    "padel['Cobertura'] = 1.0\n",
    "\n",
    "# estatísticas de padel AD\n",
    "padel_ad = predictions.dropna(subset=['padel_ad'])\n",
    "cobertura_padel_ad = len(padel_ad.padel_ad) / len(predictions.Outcome)\n",
    "padel_ad = pd.DataFrame(stats(padel_ad.Outcome, padel_ad.padel_ad.astype(int)))\n",
    "padel_ad['Cobertura'] = round(cobertura_padel_ad, 2)\n",
    "\n",
    "##### Consenso\n",
    "\n",
    "# estatísticas consenso\n",
    "consensus = pd.DataFrame(stats(predictions.Outcome, predictions.consensus))\n",
    "consensus['Cobertura'] = 1.0\n",
    "\n",
    "# estatísticas consenso AD\n",
    "consensus_ad = predictions.dropna(subset=['consensus_ad'])\n",
    "cobertura_consensus_ad = len(consensus_ad.consensus_ad) / len(predictions.Outcome)\n",
    "\n",
    "consensus_ad = pd.DataFrame(stats(consensus_ad.Outcome, consensus_ad.consensus_ad.astype(int)))\n",
    "consensus_ad['Cobertura'] = round(cobertura_consensus_ad, 2)\n",
    "\n",
    "# estatísticas rigor do consenso\n",
    "consensus_rigor = predictions.dropna(subset=['consensus_rigor'])\n",
    "cobertura_consensus_rigor = len(consensus_rigor.consensus_rigor) / len(predictions.Outcome)\n",
    "consensus_rigor = pd.DataFrame(stats(consensus_rigor.Outcome, consensus_rigor.consensus_rigor.astype(int)))\n",
    "consensus_rigor['Cobertura'] = round(cobertura_consensus_rigor, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Previsões de exportação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_exp = predictions.drop(columns=['Mol'])\n",
    "\n",
    "with pd.ExcelWriter('predictions-sirms-padel.xlsx') as writer:\n",
    "    pred_exp.to_excel(writer, sheet_name='sirms-padel', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Estatísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.concat([sirms_ad, padel_ad, consensus, consensus_ad, consensus_rigor], axis=0)\n",
    "stats.set_index([['SiRMS', 'PaDEL', 'Consenso', 'Consenso (AD)', 'Consenso com rigor']], drop=True, inplace=True)\n",
    "\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estatísticas de transposição\n",
    "stats_t = stats.T\n",
    "stats_t = stats_t.reset_index()\n",
    "stats_t = stats_t.rename(columns={'index': 'Stats'})\n",
    "\n",
    "# Fazer plot\n",
    "plt.style.use('seaborn-colorblind')\n",
    "fig, ax1 = plt.subplots(figsize=(8,5), dpi=130)\n",
    "\n",
    "stats_t.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_xticklabels(labels=stats_t['Stats'].tolist(), fontsize=14, rotation=0)\n",
    "ax1.axhline(y=.6, color='indianred', ls='dashed')\n",
    "ax1.legend_.remove()\n",
    "plt.title('Características estatísticas do QSAR ', fontsize=16)\n",
    "ax1.set_yticks(np.arange(0, 1.1, 0.1))\n",
    "ax1.tick_params(labelsize=9)\n",
    "\n",
    "handles, labels = ax1.get_legend_handles_labels()\n",
    "ax1.legend(handles, labels, fontsize=10,\n",
    "            loc='upper center', bbox_to_anchor=(0.5, -0.09), ncol=5)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('statistics-sirms-padel-5f.png', bbox_inches='tight', transparent=False, format='png', dpi=300)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
